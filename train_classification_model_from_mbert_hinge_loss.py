"""
Automatically generated by Colaboratory.


# Prerequisits
"""


"""# Import"""
import pandas as pd
import numpy as np
import torch as tc
import os
import time
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch import nn
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import csv
if tc.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'

import transformers

from transformers import BertTokenizer, BertModel
transformers.logging.set_verbosity_error()

from torch.optim import Adam
from tqdm import tqdm
BERT_OUT=512

class BertClassifier(nn.Module):

    def __init__(self, bert, dropout=0.1):

        super(BertClassifier, self).__init__()

        self.bert = bert
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 1)
        self.softmax = nn.Softmax(dim = 1)

    def forward(self, input_ids, attention_mask):


        sentence_output, pooled_output = self.bert(input_ids= input_ids, attention_mask=attention_mask,return_dict=False)[:2]
        output = sentence_output[:,0,:]
        dropout_output = self.dropout(output)
        linear_output = self.linear(dropout_output)

        return linear_output


class TextTrainDataset(Dataset):
    def __init__(self, tokenizer, relevant_path, nonrelevant_path, queries):
        self.queries = queries
        self.tokenizer = tokenizer

        self.relevant = pd.read_csv(relevant_path, index_col = 0, dtype = {'qid' : str})
        self.nonrelevant = pd.read_csv(nonrelevant_path, index_col = 0, dtype = {'qid' : str})
        print(len(self.relevant), len(self.nonrelevant))
    def __len__(self):
        return len(self.relevant)

    def __getitem__(self, idx):
        if idx == 0:
            self.relevant = self.relevant.sample(frac=1).reset_index(drop=True)

        relevant = self.relevant.iloc[idx]
        qid = relevant['qid']
        subdf_nonrelevant = self.nonrelevant[self.nonrelevant['qid'] == qid]
        nonrelevant = subdf_nonrelevant.sample(1).iloc[0]

        r_text = relevant['text']
        n_text = nonrelevant['text']
        texts = [r_text, n_text]

        q_text = [self.queries[qid]] * 2
        item = self.tokenizer(q_text, texts, truncation = True, padding='max_length', max_length = 512, return_tensors="pt").to(device)
        return item



class TestDataset(Dataset):
  def __init__(self, tokenizer, qrels, text_path, queries):
    self.qrels = qrels
    self.text_path = text_path
    self.queries = queries
    self.tokenizer = tokenizer
    self.docnos = qrels['docno'].tolist()
    self.qids = qrels['qid'].tolist()

  def __len__(self):
    return len(self.qrels)

  def __getitem__(self, idx):


    sample = self.qrels.iloc[idx]

    def read(docno):
      docno =  docno+'.txt'

      p = self.text_path + docno

      with open(p) as f:
        text = f.read()
        return text
    text = read(sample['docno'])
    qid = sample['qid']
    
    q_text = self.queries[qid]
    p_item = self.tokenizer(q_text, text, truncation = True, padding='max_length', max_length = 512, return_tensors="pt").to(device)

    return p_item


"""# Train"""


def eval_results(scores, qrels):
  prec_at_10 = []
  prec = []
  for qid in np.unique(scores['qid']):
    positive = qrels[(qrels['qid'] == qid) & (qrels['label'] == 1)]['docno'].tolist()
    subscores = scores[scores['qid'] == qid]
    subscores = subscores.sort_values(by='score', ascending = False)
    docnos = subscores['docno'].tolist()
    at10, p = 0, 0
    for i, d in enumerate(docnos):
      if d in positive:
        if i < 10:
          at10 += 1
        if i < 50:
          p += 1
        else:
          break
    prec_at_10.append(at10/10)
    prec.append(p/50)
  print( f'Prec at 10: {np.mean(prec_at_10): .3f} | Prec at 50: {np.mean(prec): .3f}' )

def eval(model, dataset):
  with tc.no_grad():
    scores = []
    test_dataloader = DataLoader(test_dataset, batch_size = BATCH, shuffle = False)
    for input_data in tqdm(test_dataloader):
      output = model(input_ids = input_data['input_ids'].squeeze(1), attention_mask = input_data['attention_mask'].squeeze(1))
      output_scores = output.detach().cpu().tolist()
      scores += output_scores
    sim_scores = pd.DataFrame(data = {"docno": dataset.docnos[:len(dataset)], "qid": dataset.qids[:len(dataset)], "score" :  scores})

    eval_results(sim_scores, dataset.qrels)


def train(model, train_data, val_data, learning_rate, epochs, model_name):

    criterion = nn.HingeEmbeddingLoss().to(device)
    softmax = nn.Softmax(dim = 1)
    model = model.to(device)
    optimizer = Adam(model.parameters(), lr= learning_rate)

    def encode_sample(train_input):

      output = model( input_ids = train_input['input_ids'].reshape(-1, BERT_OUT), attention_mask = train_input['attention_mask'].reshape(-1, BERT_OUT))
      return output
    for epoch_num in range(epochs):

            total_loss_train = 0

            for sample in tqdm(train_data):
                

                output = encode_sample(sample)
                output = output.reshape(-1, 2)

                target = tc.tensor([-1] * output.shape[0]).to(device)
                joined = softmax(output)
                diff = joined[:,0] - joined[:,1]

                batch_loss = criterion(diff, target)
                total_loss_train += batch_loss.item()


                model.zero_grad()
                batch_loss.backward()
                optimizer.step()
                optimizer.zero_grad()


            total_loss_val = 0

            tc.save({
            'epoch': epoch_num,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            },root + 'models/checkpoints/' + model_name + '/checkpoint' + str(epoch_num) + '.pt')
            print(
                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \
                | Val Loss: {total_loss_val / len(val_data): .3f}' )
            eval(model, test_dataset)

# --- Config

root = ""

BATCH = 16

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained("bert-base-multilingual-cased").to(device)

test_topics_path = root + ''
test_dataset_path = root + ''
test_qrels_path = root + ''

train_topics_path = root + ''
train_relevant_path = root + ''
train_nonrelevant_path = root + ''

model_name = ''

# ---

topics_test = pd.read_csv(test_topics_path, index_col = 0, dtype = {'identifier':'str'})
topics_test = topics_test.rename(columns = {'identifier' : 'qid'})
topics_test['query'] = topics_test.apply(lambda x : x['title'] + '. ' + x['description'], axis = 1)
tuples_test = topics_test.apply(lambda x : (str(x['qid']), x['query']), axis = 1).tolist()
queries_test = dict(tuples_test)
test_qrels = pd.read_csv(test_qrels_path, dtype = {'qid' : str})
test_dataset = TestDataset(tokenizer, test_qrels, test_dataset_path, queries_test)

topics = pd.read_csv(train_topics_path, index_col = 0, dtype = {'qid' : str})
topics = topics.rename(columns={'identifier' : 'qid'})
topics['query'] = topics.apply(lambda x : (x.title + '. ' + x.description).replace('?','').replace('/','').replace("'", " "), axis=1)
tuples = topics.apply(lambda x : (str(x['qid']), x['query']), axis = 1).tolist()
queries_train = dict(tuples)
train_dataset = TextTrainDataset(tokenizer, train_relevant_path, train_nonrelevant_path, queries_train)

train_dataloader = DataLoader(train_dataset, batch_size = BATCH, shuffle= True)

EPOCHS = 20
model = BertClassifier(model).to(device)
LR = 1e-5
eval(model, test_dataset)
train(model, train_dataloader, test_dataset, LR, EPOCHS, model_name)

tc.save(model.state_dict(), root + 'models/' + model_name + '.pt')


                            
